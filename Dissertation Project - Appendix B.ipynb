{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the DOHA Model Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please make sure you have followed the instructions from https://github.com/Starstorm/Dissertation before attempting to run any cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (0.23.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: keras in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: gensim in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied: six in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from sklearn) (0.20.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from matplotlib) (2.3.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied: requests in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
      "Requirement already satisfied: boto3 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from smart-open>=1.7.0->gensim) (1.9.106)\n",
      "Requirement already satisfied: setuptools in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (40.6.3)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.106 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.106)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\joshl\\anaconda3\\envs\\pykeras\\lib\\site-packages (from botocore<1.13.0,>=1.12.106->boto3->smart-open>=1.7.0->gensim) (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas nltk numpy sklearn keras gensim seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If pre-trained model is present, load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\joshl\\Anaconda3\\envs\\pykeras\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\joshl\\Anaconda3\\envs\\pykeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\joshl\\Anaconda3\\envs\\pykeras\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"3_3_2019_acc_9751.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DataFrame, set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_3_clearance_data.csv\")\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing data, create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['FULL_TEXT'], df['Decision'], test_size=0.2, random_state=42)\n",
    "NUM_WORDS=20000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert output to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = df['Decision'].unique()\n",
    "dic={}\n",
    "for i,choice in enumerate(choices):\n",
    "    dic[choice]=i\n",
    "Y_train_binary=Y_train.apply(lambda x:dic[x])\n",
    "Y_test_binary=Y_test.apply(lambda x:dic[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Define and Apply check_gender function, create two new DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gender(text):\n",
    "    global bad_count\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    male_references = tokenized_words.count(\"he\") + tokenized_words.count(\"He\") + tokenized_words.count(\"him\") + tokenized_words.count(\"Him\")\n",
    "    female_references = tokenized_words.count(\"she\") + tokenized_words.count(\"She\") + tokenized_words.count(\"her\") + tokenized_words.count(\"Her\")\n",
    "    # While imperfect, research showed that these rules accurately determined gender and minimized throw-away cases\n",
    "    if (male_references >= 3 or female_references >= 3) and ((male_references == 0 or female_references == 0) or (((male_references/female_references) > 1.5) or ((female_references/male_references) > 1.5))):\n",
    "        if male_references > female_references:\n",
    "            return 1\n",
    "        elif female_references > male_references:\n",
    "            return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "df['is_male'] = df['FULL_TEXT'].apply(check_gender)\n",
    "\n",
    "df_male = df[df['is_male'] == 1]\n",
    "df_female = df[df['is_male'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_current in [df_male,df_female]:\n",
    "    X_train_current, X_test_current, Y_train_current, Y_test_current = train_test_split(df_current['FULL_TEXT'], df_current['Decision'], test_size=0.2, random_state=42)\n",
    "    choices = df['Decision'].unique()\n",
    "    dic={}\n",
    "    for i,choice in enumerate(choices):\n",
    "        dic[choice]=i\n",
    "    Y_train_current_binary=Y_train_current.apply(lambda x:dic[x])\n",
    "    Y_test_current_binary=Y_test_current.apply(lambda x:dic[x])\n",
    "    \n",
    "    sequences_train = tokenizer.texts_to_sequences(X_train_current)\n",
    "    sequences_valid = tokenizer.texts_to_sequences(X_test_current)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    X_train_current = pad_sequences(sequences_train)\n",
    "    X_val_current = pad_sequences(sequences_valid,maxlen=X_train_current.shape[1])\n",
    "    y_train_current = to_categorical(np.asarray(Y_train_current_binary))\n",
    "    y_val_current = to_categorical(np.asarray(Y_test_current_binary))\n",
    "    \n",
    "    sequences_test=tokenizer.texts_to_sequences(X_test_current)\n",
    "    X_test_current = pad_sequences(sequences_test, maxlen=5514)\n",
    "    y_pred_current = model.predict(X_test_current)\n",
    "    \n",
    "    calc_stats(Y_test_current, y_pred_current)\n",
    "    \n",
    "    print(TP)\n",
    "    print(FP)\n",
    "    print(TN)\n",
    "    print(FN)\n",
    "\n",
    "    accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "    recall = TP/(TP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    f1_score = 2 * (recall * precision)/(recall + precision)\n",
    "\n",
    "    print(accuracy)\n",
    "    print(recall)\n",
    "    print(precision)\n",
    "    print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code was originally from https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights and has been modified to fit this use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format and Shape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26778 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_valid=tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train and X validation tensor: (4004, 5514) (1002, 5514)\n",
      "Shape of label train and validation tensor: (4004, 2) (1002, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "y_train = to_categorical(np.asarray(Y_train_binary))\n",
    "y_val = to_categorical(np.asarray(Y_test_binary))\n",
    "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
    "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train New Model (only run these cells if you did NOT load the pretrained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Word2Vec model for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the filepath below to where your Word2Vec model is stored\n",
    "word_vectors = KeyedVectors.load_word2vec_format(r'D:\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_val, y_val),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"your_new_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whether you just trained a model or you used the pre-trained model, continue here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Statistics from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(Y_test, y_pred):\n",
    "    global TP, FP, TN, FN\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    Y_test_list = Y_test.tolist()\n",
    "\n",
    "    for idx, elem in enumerate(y_pred):\n",
    "        if Y_test_list[idx] == \"Clearance denied\":\n",
    "            answer = False\n",
    "        elif Y_test_list[idx] == \"Clearance granted\":\n",
    "            answer = True\n",
    "        else:\n",
    "            print(Y_test[idx])\n",
    "            assert False\n",
    "        if elem[0] >= 0.5:\n",
    "            if answer == False:\n",
    "                TN += 1\n",
    "            elif answer == True:\n",
    "                FN += 1\n",
    "        elif elem[0] < 0.5:\n",
    "            if answer == False:\n",
    "                FP += 1\n",
    "            elif answer == True:\n",
    "                TP += 1\n",
    "                \n",
    "calc_stats(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display All Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive: 315\n",
      "False Positive: 11\n",
      "True Negative: 662\n",
      "False Negative: 14\n",
      "Accuracy: 0.9750499001996008\n",
      "Recall: 0.9574468085106383\n",
      "Precision: 0.9662576687116564\n",
      "F1 Score: 0.9618320610687022\n",
      "Informedness: 0.941102083399197\n",
      "Markedness: 0.9455476095400588\n"
     ]
    }
   ],
   "source": [
    "print(\"True Positive: {}\".format(TP))\n",
    "print(\"False Positive: {}\".format(FP))\n",
    "print(\"True Negative: {}\".format(TN))\n",
    "print(\"False Negative: {}\".format(FN))\n",
    "\n",
    "accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "recall = TP/(TP + FN)\n",
    "precision = TP/(TP + FP)\n",
    "f1_score = 2 * (recall * precision)/(recall + precision)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"F1 Score: {}\".format(f1_score))\n",
    "\n",
    "informedness = recall - (FP/(FP + TN))\n",
    "markedness = precision - (FN/(TN + FN))\n",
    "\n",
    "print(\"Informedness: {}\".format(informedness))\n",
    "print(\"Markedness: {}\".format(markedness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare other ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5006, 73380)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df['FULL_TEXT']).toarray()\n",
    "labels = df['Decision']\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0)]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "        \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Visualization of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
